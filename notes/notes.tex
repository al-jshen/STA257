\documentclass[a4paper,10pt]{article}
\usepackage{geometry}
\usepackage{graphicx}
\graphicspath{ {./figures/} }
\usepackage{caption}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{mathtools}
\usepackage{amsmath,amsthm,amssymb,cancel,bm,upgreek}
\usepackage{floatrow}
\usepackage[makeroom]{cancel}
\usepackage{lastpage}
\geometry{total={210mm,297mm},
left=25mm,right=25mm,%
bindingoffset=0mm, top=20mm,bottom=20mm}
\setlength{\parindent}{0pt}
\newcommand{\linia}{\rule{\linewidth}{0.5pt}}
\newcommand{\fig}[1]{\centerline{\includegraphics[width=0.6\columnwidth]{#1}}}
\newcommand{\ind}{\leavevmode{\parindent=1em\indent}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}

\AtBeginDocument{%
  \setlength\abovedisplayskip{-3pt}
  \setlength\belowdisplayskip{5pt}}
   

% title configuration
\makeatletter
\def\@maketitle{%
\begin{center}
\hfill{\textit{Last modified \today}} \\
\vspace{1em}
{\huge \textsc{\@title}\par}
\vspace{1em}
\\
\linia\\
\vspace{1em}
\@author
\vspace{1em}
\end{center}
}
\makeatother

% header, footer configuration
\usepackage{fancyhdr,lastpage}
\pagestyle{fancy}
\lhead{}
\chead{}
\rhead{}
\renewcommand{\headrulewidth}{0pt}
\lfoot{}
\cfoot{}
\rfoot{Page \thepage\ /\ \pageref{LastPage}}

\usepackage[hidelinks]{hyperref}

% --------------------------------------------- %
\begin{document}
\setstretch{1.125}
\title{STA257: Probability and Statistics I\\
    \Large University of Toronto | Fall 2019}
\author{Jeff Shen}
\date{\today}
\maketitle
\tableofcontents


% --------------------------------------------- %
% Probability and Counting %
% --------------------------------------------- %

\newpage
\section{Probability and Counting}

\subsection{Introduction}

\textbf{Experiments}: situations wherre outcome is random. eg. flipping a coin is an experiment.

\textbf{Sample space}: set of all possible outcomes, denoted $S$ or $\Omega$. The number of elements in the sample space (cardinality) is denoted $|\Omega|$.

\textbf{Event}: a subset of a sample space.

\textbf{Outcome}: a particular element of a sample space $s_1 \in \Omega$.

\subsection{Set Theory}

\subsubsection{Definitions}

\begin{itemize}
    \item \textbf{union}: $A \cup B$. Elements in either $A$ or $B$.
    \item \textbf{intersection}: $A \cap B$. Elements in both $A$ and $B$. 
    \item \textbf{complement} of A: $A^c$. Elements not in $A$.
    \item \textbf{empty set}: $\varnothing$. Set with no elements in it. 
    \item $A$ and $B$ are \textbf{disjoint}: $A \cap B = \varnothing$. There are no elements in the intersection of $A$ and $B$.
\end{itemize}

\subsubsection{Laws of Set Theory} 

\begin{enumerate}
    \item \textbf{commutativity}: $A \cup B = B \cup A$, $A \cap B = B \cap A$
    \item \textbf{associativity}: $(A\cup B)\cup C = A\cup (B\cup C)$, $(A\cap B)\cap C = A\cap (B\cap B)$
    \item \textbf{distributivity}: $(A\cup B)\cap C = (A\cap C)\cup (B\cap C)$, $(A\cap B)\cup C = (A\cup C)\cap (B\cup C)$
\end{enumerate}

\subsection{Probability Measures}

\textbf{Probability measure}: a function which maps subsets of $\Omega$, which can be defined on any space, to real numbers $\R$.

\subsubsection{Axioms of Probability Measures}

\begin{itemize}
    \item $P(\Omega) = 1$
    \item $\forall A \in \Omega, P(A) \geq 0$
    \item if $A_1, A_2, \ldots A_n, \ldots$ are mutually disjoint, then $P(\bigcup_{i=1}^{\infty}A_i) = \sum_{i=1}^{\infty}P(A_i)$.
\end{itemize}

\subsubsection{Properties of Probability Measures}

\begin{itemize}
    \item $\forall A \in \Omega, P(A^c) = 1 - P(A)$

        Proof:

        \begin{align*}
            1 &= P(\Omega)\;&\text{by Axiom 1} \\
              &= P(A\cup A^c)\;&\text{by definition of complement} \\
              &= P(A) + P(A^c)\;&\text{by Axiom 3 (since $A, A^c$ are disjoint)} 
        \end{align*}

        Rearrange this to see that $P(A^c) = 1 - P(A)$. 

    \item $P(\varnothing) = 0$

        Proof: 

        \begin{align*}
            P(\Omega) &= P(\Omega \cup \varnothing)\;&\text{since $\Omega \cup \varnothing = \Omega$} \\
                      &= P(\Omega) + P(\varnothing)\;&\text{by Axiom 3 (since $\Omega, \varnothing$ are disjoint)}
        \end{align*}

        So $P(\varnothing) = 0$.

    \item For $A, B \subseteq \Omega, A \subseteq B \implies P(A) \leq P(B)$

        Proof:

        \begin{align*}
            P(B) &= P(A\cup (B\cap A^c)) \\
                 &= P(A) + P(B\cap A^c)\;&\text{by Axiom 3 (since $A, A^c$ are disjoint)}
        \end{align*}
        
        But note that $P(B\cap A^c) \geq 0$ by Axiom 2. 

        Then $P(B) = P(A) + P(B\cap A^c) \geq P(A)$. 

    \item For $A, B \subseteq \Omega, P(A\cup B) = P(A) + P(B) - P(A\cap B)$

        Proof:

        \ind Case 1: $A, B$ are disjoint. Then $A\cap B = \varnothing \implies P(A\cap B) = 0$. 

        \begin{align*}
            P(A\cup B) &= P(A) + P(B)\;&\text{by Axiom 3 (since $A, B$ are disjoint)} \\
                       &= P(A) + P(B) + P(A\cap B)\;&\text{since we can add 0 wherever we want}
        \end{align*}

        \ind Case 2: $A, B$ not disjoint. Then $A\cap B \neq \varnothing$.

        \ind \ind Let $C = A\cap B^c$, $D = A\cap B$, $E = A^c\cap B$. 

        \ind \ind Then $C, D, E$ are disjoint, and $A = C\cup D$, $B = D\cup E$, and $A\cup B = C\cup D\cup E$. 

        \fig{addition_law}
        
        \begin{align*}
            P(A) + P(B) - P(A\cap B) &= P(C\cup D) + P(D\cup E) - P(D)\;&\text{by how we defined $C, D, E$}\\
                                     &= P(C) + P(D) + P(D) + P(E) - P(D)\;&\text{by Axiom 3 and disjointness of $C, D, E$}\\
                                     &= P(C) + P(D) + P(E) \\
                                     &= P(C\cup D\cup E)\;&\text{by Axiom 3 and disjointness of $C, D, E$}\\
                                     &= P(A\cup B)
        \end{align*}
\end{itemize} 

\subsection{Counting}

\textbf{Multiplication principle}: if there are $m$ ways to do one thing, and $n$ ways to do another thing, then there are $mn$ ways to do both things. 

\textbf{Permutation}: ordered arrangement of objects. 

\begin{itemize}
    \item Sampling \textbf{with replacement} means that duplicate item selection is allowed. (can pick the same object twice). For a set of size $n$ and a \textbf{sample size} (number of items selected) $r$, there are $n^r$ possible selections. 

    \item Sampling \textbf{without replacement} means that each item is selected once at most. For a set of size $n$ and a sample size $r$, there are $n(n-1)\ldots(n-r+1) = \frac{n!}{(n-r)!}$ possible selections. In particular, there are $n(n-1)\ldots(1)=n!$ ways to order $n$ elements. 
\end{itemize}

\textbf{Combination}: arrangement of objects {\em without regard to order}. Think about this as the ways to select objects without replacement, divided by the ways that those objects can be ordered. For a set of size $n$ and a sample size $r$, we express the combination as follows:

\begin{align*}
    {n\choose r} = \frac{n(n-1)\ldots(n-r+1)}{r!} = \frac{n!}{(n-r)!\,r!}
\end{align*}

\textbf{Binomial expansion}: 

\begin{align*}
    (a+b)^n = \sum_{k=0}^n{n\choose k}a^k\,b^{n-k}
\end{align*}

In particular, for $a=b=1$, 

\begin{align*}
    (1+1)^n = 2^n = \sum_{k=0}^n{n\choose k}(1)^k(1)^{n-k} = \sum_{k=0}^n{n\choose k}
\end{align*}

The number of ways to group $n$ objects into $r$ classes, with $n_i$ objects in the $i$th classes, where $1<i<r$, is given by the following formula:

\begin{align*}
    {n\choose n_1 n_2\ldots n_r} = \frac{n!}{n_1!\,n_2!\ldots n_r!}
\end{align*}

Proof: There are $\displaystyle {n\choose n_1}$ ways to select the first class, $\displaystyle {n-n_1\choose n_2}$ ways to select the second class, and so on. Repeat this for all $r$ classes, and then apply the multiplication rule. Then we have that

\begin{align*}
    {n\choose n_1}{n-n_1\choose n_2}\ldots {n-n_1-\cdots-n_{r-1}\choose n_r} &= \frac{n!}{n_1!\,(n-n_1)!}\,\frac{(n-n_1)!}{n_2!\,(n-n_1-n_2)!}\ldots \frac{(n-n_1-\cdots-n_{r-1})!}{n_r!\,0!} \\
                                                                             &= \frac{n!}{n_1!\,\cancel{(n-n_1)!}}\,\frac{\cancel{(n-n_1)!}}{n_2!\,\cancel{(n-n_1-n_2)!}}\ldots \frac{\cancel{(n-n_1-\cdots-n_{r-1})!}}{n_r!\,0!} \\
                                                                             &= \frac{n!}{n_1!\,n_2!\ldots n_r!}
\end{align*}

\subsection{Conditional Probability and Independence}

\textbf{Conditional probability}: probability that some event will occur given that another event has occured:

\begin{align*}
    P(A|B) = \frac{P(A\cap B)}{P(B)}
\end{align*}

Two events are \textbf{independent} if knowing that one has occured gives no information about the likelihood of the other occuring. Events $A, B$ are independent if and only if any of the following hold: 

\begin{itemize}
    \item $P(A|B) = P(A)$
    \item $P(B|A) = P(B)$
    \item $(A\cap B) = P(A)P(B)$
\end{itemize}

From the definition of conditional probability, we can derive \textbf{Bayes' Rule}, which describes the probability of an event given prior knowledge of conditions related to the event: 

\begin{align*}
    &P(A|B)\,P(B) = P(A\cap B) = P(B\cap A) = P(B|A)\,P(A) \\[0.1cm]
    \implies &P(A|B)\,P(B) = P(B|A)\,P(A) \\[0.1cm]
    \implies &P(A|B) = \frac{P(B|A)\,P(A)}{P(B)} 
\end{align*}

\subsection{Law of Total Probability}

Let $B_1, B_2, \ldots, B_n$ be sets satisfying the following conditions:

\begin{enumerate}
    \item $B_1\cup B_2\cup \ldots \cup B_n = \Omega$ (the $B_i$'s are a \textbf{partition} of $\Omega$)
    \item $\forall i, j \in [1, n]\subseteq \N, i\neq j \implies B_i\cap B_j = \varnothing$ (pairwise disjoint)
    \item $\forall i \in [1, n]\subseteq \N, P(B_i) > 0$ (strictly positive probability),
\end{enumerate}

we can conclude that for any event $A$, we have that 

\begin{align*}
    P(A) = \sum_{i=1}^n P(A|B_i)\,P(B_i)
\end{align*}

Proof: Since all $B_i$'s are pairwise disjoint, all $A\cap B_i$'s must also be pairwise disjoint. Then 

\begin{align*}
    P(A) &= P(A\cap \Omega) \\
         &= P\left(A\cap \left(\bigcup_{i=1}^n B_i\right)\right)\,&\text{by condition 1}\\
         &= P\left(\bigcup_{i=1}^n \left(A\cap B_i\right)\right) \\
         &= \sum_{i=1}^n P(A\cap B_i)\,&\text{by Axiom 3 of probability measures}\\
         &= \sum_{i=1}^n P(A|B_i)\,P(B_i)\,&\text{by definition of conditional probability}
\end{align*}

Under the same conditions, we can also conclude an alternate form of Bayes' Rule:

\begin{align*}
    P(B_j|A) = \frac{P(A|B_j)\,P(B_j)}{\sum_{i=1}^n P(A|B_i)\,P(B_i)}
\end{align*}

\subsection{Equations}

\begin{tabularx}{\textwidth}{ l X r }
    Axioms && $P(\Omega) = 1 \\[0.2cm]
           && $P(A) \geq 0 \\[0.2cm]
           && for mutually disjoint sets, $P(\bigcup_{i=1}^{\infty}A_i) = \sum_{i=1}^{\infty}P(A_i)$ \\[0.2cm]

    Properties && $P(A^c) = 1-P(A)$ \\[0.2cm]
               && $P(\varnothing) = 0, P(\Omega) = 1$ \\[0.2cm]
               && $A\subseteq B \implies P(A) \leq P(B)$ \\[0.2cm]
               && $P(A\cup B) = P(A) + P(B) - P(A\cap B)$\\[0.2cm]

    Permutations && with replacement: $n^r$ \\[0.2cm]
                 && without replacement: $\frac{n!}{(n-r)!}$ \\[0.2cm]

    Combinations && ${n\choose r} = \frac{n!}{(n-r)!\,r!}$ \\[0.3cm]

    Binomial Expansion && $(a+b)^n = \sum_{i=1}^{n}{n\choose k}a^k\,b^{n-k}$ \\[0.3cm]

    Grouping $n$ objects into $r$ classes && ${n\choose n_1 n_2\ldots n_r} = \frac{n!}{n_1!\,n_2!\ldots n_r!}$ \\[0.3cm]

    Conditional probability && $P(A|B) = \frac{P(A\cap B)}{P(B)}$ \\[0.3cm]

    Bayes' Rule && $P(A|B) = \frac{P(B|A)\,P(A)}{P(B)}$ \\[0.3cm]
                && $P(B_j|A) = \frac{P(A|B_j)\,P(B_j)}{\sum_{i=1}^n P(A|B_i)\,P(B_i)}$ \\[0.3cm]

    Law of Total Probability && $P(A) = \sum_{i=1}^n P(A|B_i)\,P(B_i)$ \\
 
\end{tabularx}

% --------------------------------------------- %
% Random Variables %
% --------------------------------------------- %

\newpage
\section{Random Variables}

% --------------------------------------------- %
\subsection{Discrete Random Variables}

\textbf{Random variable}: function from $\Omega$ to $\R$. 

\textbf{Discrete random variable}: random variable which can only take a finite number of values, or a countably infinite number of values. 

\textbf{Probability mass function (PMF)}: describes probability properties of a random variable. For some discrete random variable $X$ taking on values $x_1, x_2, \ldots$, the probability mass function is some function $p$ such that $p(x_i) = P(X=x_i)$, and satisfies that $\sum_{i=1}^{\infty} p(x_i) = 1$.

\textbf{Cumulative distribution function (CDF)}: defined as some function $F$ such that $F(x) = P(X \leq x)$, where $-\infty < x < \infty$. It satisfies the following conditions:

\begin{itemize}
    \item $\lim_{x\to-\infty} F(x) = 0$
    \item $\lim_{x\to\infty} F(x) = 1$
    \item is right-continuous (can have jumps, but must be continuous when approaching values from the right side). Continuity implies right-continuity. 
    \item is non-decreasing. 
\end{itemize}

\fig{pmfcdf}

For random variables $X, Y$ taking on values $x_1, x_2, \ldots$ and $y_1, y_2, \ldots$ respectively, $X$ and $Y$ are \textbf{independent} if $\forall i, j, P(X=x_i, Y=y_j) = P(X=x_i)P(Y=y_j)$. 

\subsubsection{Bernoulli}

$X \sim$ Bern($p$). Was there a success?

$X$ takes on the values $1$ and $0$ with probabilities $p$ and $q=1-p$ respectively. For some event $A$, Bernoulli random variables are often used as an \textbf{indicator random variable}. That is, $I_A=1$ if $A$ occurs, and $I_A=0$ otherwise. 

\subsubsection{Binomial}

$X \sim$ Bin($n, p$). Something was done $n$ times. How many successes were there?

Binomial random variables can be thought of as the sum of $n$ \textbf{i.i.d (independent and identically distributed)} Bernoulli random variables. That is, $X = X_1 + X_2 + \cdots + X_n$, where $X_i \sim$ Bern($p$).

\subsubsection{Geometric}

$X \sim$ Geom($p$). Keep going until there is a success. 

Constructed from a (countably) infinite sequence of Bernoulli trials. Count the number of trials it took until finally getting a success. Some people don't include the $k$th trial (the last trial, which is a success), but we will. So $k$ is the trial on which we have a success.

\subsubsection{Negative Binomial}

$X \sim$ NB($n, p$). Keep going until there are $n$ successes.

A natural extension to the geometric distribution—can be thought of as the sum of $n$ i.i.d geometric random variables. That is, $X = X_1 + X_2 + \cdots + X_n$, where $X_i \sim$ Geom($p$).

\subsubsection{Hypergeometric}

$X \sim$ HG($n, r, m$). Total population of $n$ in which $r$ are marked/tagged. We randomly pick $m$ from the population without replacement. How many are tagged?

\subsubsection{Poisson}

$X \sim$ Pois($\lambda$). Large number of trials, each will a small probability of success. 

$\lambda$ is called the \textbf{rate parameter}. Think of dividing some interval into a large number of subintervals, such that the probability of an event occuring in each subinterval is small, but there are a large number of subintervals. 

Assume that what happens in one subinterval is independent of what happens in the other subintervals, that the probability of some event occuring is the same for each subinterval, and that events do not occur simultaneously. Then this can be derived by setting up a binomial distribution in such a way that $n \to \infty$, $p\to 0$, and $np=\lambda$. 

% --------------------------------------------- %
\subsection{Continuous Random Variables}

\subsubsection{Uniform}

\subsubsection{Exponential}

\subsubsection{Gamma}

\subsubsection{Beta}

\subsubsection{Uniform}

\subsubsection{Standard Normal}

\subsubsection{General Normal}

\subsection{Transformations of Random Variables}

\subsection{Distributions}
pmf/pdf, cdf, ex, var, range of support for k
% --------------------------------------------- %
% Expected Values %
% --------------------------------------------- %

\newpage
\section{Expected Values}

\subsection{Mean and Variance}
\subsubsection{LOTUS}
\subsubsection{Inequalities}

\subsection{Moment Generating Functions}


% --------------------------------------------- %
% Joint Distributions %
% --------------------------------------------- %

\newpage
\section{Joint Distributions}

\subsection{Joint and Marginal Distributions}
\subsubsection{Discrete}
\subsubsection{Continuous}

\subsection{Independence in Joint Distributions}

\subsection{Conditional Distributions}
\subsubsection{Discrete}
\subsubsection{Continuous}

\subsection{Functions of Joint Distributions}

\subsection{Order Statistics}

\end{document}
