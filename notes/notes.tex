\documentclass[a4paper,10pt]{article}
\usepackage{geometry}
\usepackage{graphicx}
\graphicspath{{./figures/}}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{mathtools}
\usepackage{amsmath,amsthm,amssymb,cancel,bm,upgreek}
\usepackage{floatrow}
\usepackage{cancel}
\usepackage{sidecap}
\usepackage{lastpage}
\usepackage{lscape}
\geometry{total={210mm,297mm},
left=25mm,right=25mm,%
bindingoffset=0mm, top=20mm,bottom=20mm}

\newcommand{\linia}{\rule{\linewidth}{0.5pt}}
\newcommand{\fig}[1]{\centerline{\includegraphics[width=0.6\columnwidth]{#1}}}
\newcommand{\ind}{\leavevmode{\parindent=1em\indent}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}

\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\SD}{\mathrm{SD}}

\setcounter{tocdepth}{2}
\setlength{\parindent}{0pt}
\AtBeginDocument{%
  \setlength\abovedisplayskip{-3pt}
  \setlength\belowdisplayskip{5pt}}
   

% title configuration
\makeatletter
\def\@maketitle{%
\begin{center}
\hfill{\textit{Last modified \today}} \\
\vspace{1em}
{\huge \textsc{\@title}\par}
\vspace{1em}
\linia
\vspace{1em}
\@author
\vspace{1em}
\end{center}
}
\makeatother

% header, footer configuration
\usepackage{fancyhdr,lastpage}
\pagestyle{fancy}
\lhead{}
\chead{}
\rhead{}
\renewcommand{\headrulewidth}{0pt}
\lfoot{}
\cfoot{}
\rfoot{Page \thepage\ /\ \pageref{LastPage}}

\usepackage[hidelinks]{hyperref}

% --------------------------------------------- %
\begin{document}
\setstretch{1.125}
\title{STA257: Probability and Statistics I\\
    \Large University of Toronto | Fall 2019}
\author{Jeff Shen}
\date{\today}
\maketitle
\tableofcontents


% --------------------------------------------- %
% Probability and Counting %
% --------------------------------------------- %

\newpage
\section{Probability and Counting}

\subsection{Introduction}

\textbf{Experiments}: situations where outcome is random.\ e.g.\ flipping a coin is an experiment.

\textbf{Sample space}: set of all possible outcomes, denoted $S$ or $\Omega$. The number of elements in the sample space (cardinality) is denoted $|\Omega|$.

\textbf{Event}: a subset of a sample space.

\textbf{Outcome}: a particular element of a sample space.\ e.g.\ $s_1 \in \Omega$.

\subsection{Set Theory}

\subsubsection{Definitions}

\begin{itemize}
    \item \textbf{union}: $A \cup B$. Elements in either $A$ or $B$.
    \item \textbf{intersection}: $A \cap B$. Elements in both $A$ and $B$. 
    \item \textbf{complement} of A: $A^c$. Elements not in $A$.
    \item \textbf{empty set}: $\varnothing$. Set with no elements in it.
    \item $A$ and $B$ are \textbf{disjoint}: $A \cap B = \varnothing$. There are no elements in the intersection of $A$ and $B$.
\end{itemize}

\subsubsection{Laws of Set Theory} 

\begin{enumerate}
    \item \textbf{commutativity}: $A \cup B = B \cup A$, $A \cap B = B \cap A$
    \item \textbf{associativity}: $(A\cup B)\cup C = A\cup (B\cup C)$, $(A\cap B)\cap C = A\cap (B\cap B)$
    \item \textbf{distributivity}: $(A\cup B)\cap C = (A\cap C)\cup (B\cap C)$, $(A\cap B)\cup C = (A\cup C)\cap (B\cup C)$
\end{enumerate}

\subsection{Probability Measures}

\textbf{Probability measure}: a function which maps subsets of $\Omega$, which can be defined on any space, to real numbers $\R$.

\subsubsection{Axioms of Probability Measures}

\begin{itemize}
    \item $P(\Omega) = 1$
    \item $\forall A \in \Omega, P(A) \geq 0$
    \item if $A_1, A_2, \ldots A_n, \ldots$ are mutually disjoint, then $P(\bigcup_{i=1}^{\infty}A_i) = \sum_{i=1}^{\infty}P(A_i)$.
\end{itemize}

\subsubsection{Properties of Probability Measures}

\begin{itemize}
    \item $\forall A \in \Omega, P(A^c) = 1 - P(A)$

        Proof:

        \begin{align*}
            1 &= P(\Omega)\;&\text{by Axiom 1} \\
              &= P(A\cup A^c)\;&\text{by definition of complement} \\
              &= P(A) + P(A^c)\;&\text{by Axiom 3 (since $A, A^c$ are disjoint)} 
        \end{align*}

        Rearrange this to see that $P(A^c) = 1 - P(A)$. 

    \item $P(\varnothing) = 0$

        Proof: 

        \begin{align*}
            P(\Omega) &= P(\Omega \cup \varnothing)\;&\text{since $\Omega \cup \varnothing = \Omega$} \\
                      &= P(\Omega) + P(\varnothing)\;&\text{by Axiom 3 (since $\Omega, \varnothing$ are disjoint)}
        \end{align*}

        So $P(\varnothing) = 0$.

    \item For $A, B \subseteq \Omega, A \subseteq B \implies P(A) \leq P(B)$

        Proof:

        \begin{align*}
            P(B) &= P(A\cup (B\cap A^c)) \\
                 &= P(A) + P(B\cap A^c)\;&\text{by Axiom 3 (since $A, A^c$ are disjoint)}
        \end{align*}
        
        But note that $P(B\cap A^c) \geq 0$ by Axiom 2. 

        Then $P(B) = P(A) + P(B\cap A^c) \geq P(A)$. 

    \item For $A, B \subseteq \Omega, P(A\cup B) = P(A) + P(B) - P(A\cap B)$

        Proof:

        \ind Case 1: $A, B$ are disjoint. Then $A\cap B = \varnothing \implies P(A\cap B) = 0$. 

        \begin{align*}
            P(A\cup B) &= P(A) + P(B)\;&\text{by Axiom 3 (since $A, B$ are disjoint)} \\
                       &= P(A) + P(B) + P(A\cap B)\;&\text{since we can add 0 wherever we want}
        \end{align*}

        \ind Case 2: $A, B$ not disjoint. Then $A\cap B \neq \varnothing$.

        \ind \ind Let $C = A\cap B^c$, $D = A\cap B$, $E = A^c\cap B$. 

        \ind \ind Then $C, D, E$ are disjoint, and $A = C\cup D$, $B = D\cup E$, and $A\cup B = C\cup D\cup E$. 

        \fig{addition_law}
        
        \begin{align*}
            P(A) + P(B) - P(A\cap B) &= P(C\cup D) + P(D\cup E) - P(D)\;&\text{by how we defined $C, D, E$}\\
                                     &= P(C) + P(D) + P(D) + P(E) - P(D)\;&\text{by Axiom 3 and disjointness of $C, D, E$}\\
                                     &= P(C) + P(D) + P(E) \\
                                     &= P(C\cup D\cup E)\;&\text{by Axiom 3 and disjointness of $C, D, E$}\\
                                     &= P(A\cup B)
        \end{align*}
\end{itemize} 

\subsection{Counting}

\textbf{Multiplication principle}: if there are $m$ ways to do one thing, and $n$ ways to do another thing, then there are $mn$ ways to do both things. 

\textbf{Permutation}: ordered arrangement of objects. 

\begin{itemize}
    \item Sampling \textbf{with replacement} means that duplicate item selection is allowed. (can pick the same object twice). For a set of size $n$ and a \textbf{sample size} (number of items selected) $r$, there are $n^r$ possible selections. 

    \item Sampling \textbf{without replacement} means that each item is selected once at most. For a set of size $n$ and a sample size $r$, there are $n(n-1)\ldots(n-r+1) = \frac{n!}{(n-r)!}$ possible selections. In particular, there are $n(n-1)\ldots(1)=n!$ ways to order $n$ elements. 
\end{itemize}

\textbf{Combination}: arrangement of objects {\em without regard to order}. Think about this as the ways to select objects without replacement, divided by the ways that those objects can be ordered. For a set of size $n$ and a sample size $r$, we express the combination as follows:

\begin{align*}
    {n\choose r} = \frac{n(n-1)\ldots(n-r+1)}{r!} = \frac{n!}{(n-r)!\,r!}
\end{align*}

\textbf{Binomial expansion}: 

\begin{align*}
    (a+b)^n = \sum_{k=0}^n{n\choose k}a^k\,b^{n-k}
\end{align*}

In particular, for $a=b=1$, 

\begin{align*}
    (1+1)^n = 2^n = \sum_{k=0}^n{n\choose k}(1)^k(1)^{n-k} = \sum_{k=0}^n{n\choose k}
\end{align*}

The number of ways to group $n$ objects into $r$ classes, with $n_i$ objects in the $i$th classes, where $1<i<r$, is given by the following formula:

\begin{align*}
    {n\choose n_1 n_2\ldots n_r} = \frac{n!}{n_1!\,n_2!\ldots n_r!}
\end{align*}

Proof: There are $\displaystyle {n\choose n_1}$ ways to select the first class, $\displaystyle {n-n_1\choose n_2}$ ways to select the second class, and so on. Repeat this for all $r$ classes, and then apply the multiplication rule. Then we have that

\begin{align*}
    {n\choose n_1}{n-n_1\choose n_2}\ldots {n-n_1-\cdots-n_{r-1}\choose n_r} &= \frac{n!}{n_1!\,(n-n_1)!}\,\frac{(n-n_1)!}{n_2!\,(n-n_1-n_2)!}\ldots \frac{(n-n_1-\cdots-n_{r-1})!}{n_r!\,0!} \\
                                                                             &= \frac{n!}{n_1!\,\cancel{(n-n_1)!}}\,\frac{\cancel{(n-n_1)!}}{n_2!\,\cancel{(n-n_1-n_2)!}}\ldots \frac{\cancel{(n-n_1-\cdots-n_{r-1})!}}{n_r!\,0!} \\
                                                                             &= \frac{n!}{n_1!\,n_2!\ldots n_r!}
\end{align*}

\subsection{Conditional Probability and Independence}

\textbf{Conditional probability}: probability that some event will occur given that another event has occured:

\begin{align*}
    P(A|B) = \frac{P(A\cap B)}{P(B)}
\end{align*}

Two events are \textbf{independent} if knowing that one has occured gives no information about the likelihood of the other occuring. Events $A, B$ are independent if and only if any of the following hold: 

\begin{itemize}
    \item $P(A|B) = P(A)$
    \item $P(B|A) = P(B)$
    \item $(A\cap B) = P(A)P(B)$
\end{itemize}

From the definition of conditional probability, we can derive \textbf{Bayes' Rule}, which describes the probability of an event given prior knowledge of conditions related to the event: 

\begin{align*}
    &P(A|B)\,P(B) = P(A\cap B) = P(B\cap A) = P(B|A)\,P(A) \\[0.1cm]
    \implies &P(A|B)\,P(B) = P(B|A)\,P(A) \\[0.1cm]
    \implies &P(A|B) = \frac{P(B|A)\,P(A)}{P(B)} 
\end{align*}

\subsection{Law of Total Probability}

Let $B_1, B_2, \ldots, B_n$ be sets satisfying the following conditions:

\begin{enumerate}
    \item $B_1\cup B_2\cup \ldots \cup B_n = \Omega$ (the $B_i$'s are a \textbf{partition} of $\Omega$)
    \item $\forall i, j \in [1, n]\subseteq \N, i\neq j \implies B_i\cap B_j = \varnothing$ (pairwise disjoint)
    \item $\forall i \in [1, n]\subseteq \N, P(B_i) > 0$ (strictly positive probability),
\end{enumerate}

we can conclude that for any event $A$, we have that 

\begin{align*}
    P(A) = \sum_{i=1}^n P(A|B_i)\,P(B_i)
\end{align*}

Proof: Since all $B_i$'s are pairwise disjoint, all $A\cap B_i$'s must also be pairwise disjoint. Then 

\begin{align*}
    P(A) &= P(A\cap \Omega) \\
         &= P\left(A\cap \left(\bigcup_{i=1}^n B_i\right)\right)\,&\text{by condition 1}\\
         &= P\left(\bigcup_{i=1}^n \left(A\cap B_i\right)\right) \\
         &= \sum_{i=1}^n P(A\cap B_i)\,&\text{by Axiom 3 of probability measures}\\
         &= \sum_{i=1}^n P(A|B_i)\,P(B_i)\,&\text{by definition of conditional probability}
\end{align*}

Under the same conditions, we can also conclude an alternate form of Bayes' Rule:

\begin{align*}
    P(B_j|A) = \frac{P(A|B_j)\,P(B_j)}{\sum_{i=1}^n P(A|B_i)\,P(B_i)}
\end{align*}

\subsection{Equations}

\begin{tabularx}{\textwidth}{ l X r }
    Axioms && $P(\Omega) = 1$ \\[0.2cm]
           && $P(A) \geq 0$ \\[0.2cm]
           && for mutually disjoint sets, $P(\bigcup_{i=1}^{\infty}A_i) = \sum_{i=1}^{\infty}P(A_i)$ \\[0.2cm]

    Properties && $P(A^c) = 1-P(A)$ \\[0.2cm]
               && $P(\varnothing) = 0, P(\Omega) = 1$ \\[0.2cm]
               && $A\subseteq B \implies P(A) \leq P(B)$ \\[0.2cm]
               && $P(A\cup B) = P(A) + P(B) - P(A\cap B)$\\[0.2cm]

    Permutations && with replacement: $n^r$ \\[0.2cm]
                 && without replacement: $\frac{n!}{(n-r)!}$ \\[0.2cm]

    Combinations && ${n\choose r} = \frac{n!}{(n-r)!\,r!}$ \\[0.3cm]

    Binomial Expansion && $(a+b)^n = \sum_{i=1}^{n}{n\choose k}a^k\,b^{n-k}$ \\[0.3cm]

    Grouping $n$ objects into $r$ classes && ${n\choose n_1 n_2\ldots n_r} = \frac{n!}{n_1!\,n_2!\ldots n_r!}$ \\[0.3cm]

    Conditional probability && $P(A|B) = \frac{P(A\cap B)}{P(B)}$ \\[0.3cm]

    Bayes' Rule && $P(A|B) = \frac{P(B|A)\,P(A)}{P(B)}$ \\[0.3cm]
                && $P(B_j|A) = \frac{P(A|B_j)\,P(B_j)}{\sum_{i=1}^n P(A|B_i)\,P(B_i)}$ \\[0.3cm]

    Law of Total Probability && $P(A) = \sum_{i=1}^n P(A|B_i)\,P(B_i)$ \\
 
\end{tabularx}

% --------------------------------------------- %
% Random Variables %
% --------------------------------------------- %

\newpage
\section{Random Variables}

% --------------------------------------------- %
\subsection{Discrete Random Variables}

\textbf{Random variable}: function from $\Omega$ to $\R$. 

\textbf{Discrete random variable}: random variable which can only take a finite number of values, or a countably infinite number of values. 

\textbf{Probability mass function (PMF)}: describes probability properties of a random variable. For some discrete random variable $X$ taking on values $x_1, x_2, \ldots$, the probability mass function is some function $p$ such that $p(x_i) = P(X=x_i)$, and satisfies that $\sum_{i=1}^{\infty} p(x_i) = 1$.

\textbf{Cumulative distribution function (CDF)}: defined as some function $F$ such that $F(x) = P(X \leq x)$, where $-\infty < x < \infty$. It satisfies the following conditions:

\begin{itemize}
    \item $\lim_{x\to-\infty} F(x) = 0$
    \item $\lim_{x\to\infty} F(x) = 1$
    \item is right-continuous (can have jumps, but must be continuous when approaching values from the right side). Continuity implies right-continuity. 
    \item is non-decreasing. 
\end{itemize}

\fig{pmfcdf}

For random variables $X, Y$ taking on values $x_1, x_2, \ldots$ and $y_1, y_2, \ldots$ respectively, $X$ and $Y$ are \textbf{independent} if $\forall i, j, P(X=x_i, Y=y_j) = P(X=x_i)P(Y=y_j)$. 

\subsubsection{Bernoulli}

$X \sim$ Bern($p$). Was there a success?

$X$ takes on the values $1$ and $0$ with probabilities $p$ and $q=1-p$ respectively. For some event $A$, Bernoulli random variables are often used as an \textbf{indicator random variable}. That is, $I_A=1$ if $A$ occurs, and $I_A=0$ otherwise. 

\subsubsection{Binomial}

$X \sim$ Bin($n, p$). Something was done $n$ times. How many successes were there?

Binomial random variables can be thought of as the sum of $n$ \textbf{i.i.d (independent and identically distributed)} Bernoulli random variables. That is, $X = X_1 + X_2 + \cdots + X_n$, where $X_i \sim$ Bern($p$).

\subsubsection{Geometric}

$X \sim$ Geom($p$). Keep going until there is a success. 

Constructed from a (countably) infinite sequence of Bernoulli trials. Count the number of trials it took until finally getting a success. Some people don't include the $k$th trial (the last trial, which is a success), but we will. So $k$ is the trial on which we have a success.

\subsubsection{Negative Binomial}

$X \sim$ NB($n, p$). Keep going until there are $n$ successes.

A natural extension to the geometric distributionâ€”can be thought of as the sum of $n$ i.i.d geometric random variables. That is, $X = X_1 + X_2 + \cdots + X_n$, where $X_i \sim$ Geom($p$).

\subsubsection{Hypergeometric}

$X \sim$ HG($n, r, m$). Total population of $n$ in which $r$ are marked/tagged. We randomly pick $m$ from the population without replacement. How many are tagged?

\subsubsection{Poisson}

$X \sim$ Pois($\lambda$). Large number of trials, each will a small probability of success. 

$\lambda$ is called the \textbf{rate parameter}. Think of dividing some interval into a large number of subintervals, such that the probability of an event occuring in each subinterval is small, but there are a large number of subintervals. 

Assume that what happens in one subinterval is independent of what happens in the other subintervals, that the probability of some event occuring is the same for each subinterval, and that events do not occur simultaneously. Then this can be derived by setting up a binomial distribution in such a way that $n \to \infty$, $p\to 0$, and $np=\lambda$. \\

{\Huge DERIVE POISSON FROM BINOMIAL} 

% --------------------------------------------- %
\subsection{Continuous Random Variables}

\textbf{Continuous random variables} can take on a continuum of values rather than just a finite (or countably infinite) number. 

\textbf{Probability density function (pdf)}. Analogous to the pmf for discrete random variables. Satisfies the following conditions:

\begin{itemize}
    \item $f(x) \geq 0$
    \item $f$ is piecewise continuous 
    \item $\int_{-\infty}^{\infty} f(x)dx = 1$
\end{itemize}

For some $a, b$ such that $a<b$, the probability that $X$ is in the interval $(a, b)$ is given by the area under $f(x)$ from $a$ to $b$: $P(a < X < b) = \int_a^b f(x)dx$. Note that the probability of taking on a particular value is 0: $P(X=c) = \int_c^c f(x)dx = 0$. This implies that $P(a\leq X\leq b) = P(a<X<b)$.

For an (infinitessimally) small value of $dx$, the probability that $X$ is in the interval $(x, x+dx)$ is proportional to $f(x)$: $P(x\leq X\leq x+dx) = f(x)dx$. 

The \textbf{cumulative distribution function (cdf)} can be expressed in terms of the pdf: 
\begin{align*}
    F(x) = P(X\leq x) = \int_{-\infty}^x f(t)dt
\end{align*}

If $f$ is continuous at $x$, then by the Fundamental Theorem of Calculus, 
\begin{align*}
    f(x) &= F'(x) \\
    \implies P(a < X < b) &= \int_a^b f(x)dx = F(b) - F(a).
\end{align*}

If $F$ is strictly increasing on some interval $I$, with $F=0$ to the left of $I$ and $F=1$ to the right of $I$, then the \textbf{inverse} function $F^{-1}$ is well defined:
\begin{align*}
    y&=F(x) \\
    \implies x&=F^{-1}(y)
\end{align*}

The \textbf{$p$th quantile} of F is the value $x_p$ such that $x_p = F^{-1}(p)$. Note that the \textbf{percentile} is the same as the quantile, but expressed as a percentage from 0 to 100, whereas the quantile is expressed as a value between 0 and 1. Special quantiles are the \textbf{median ($p=\frac{1}{2}$)} and the \textbf{lower and upper quartiles ($p=\frac{1}{4}, \frac{3}{4}$)} respectively. 

\subsubsection{Uniform}

$U \sim$ Unif($a, b$). All intervals of the same length are equally probable. Probability is proportional to the length of the subinterval: longer subintervals are more probably.  

Is a "universal" distribution because it can be used to generate all other distributions. For some strictly increasing and continuous cdf $F$,
\begin{align*}
    U \sim {\rm Unif}(0, 1) &\implies X=F^{-1}(U) \sim F \\ 
    X \sim F &\implies U = F(X) \sim {\rm Unif}(0, 1)
\end{align*}
That is, plugging any random variable into its own cdf returns a uniform random variable, and plugging a uniform random variable into the inverse of the cdf yields a random variable distributed according to that cdf. 

\subsubsection{Exponential}

$X \sim$ Exp($\lambda$). Characterized by (and is the only distribution which has) the \textbf{memoryless property}: 
\begin{align*}
    P(X \geq s+t\,|\,X\geq s) = P(X\geq t)
\end{align*}
Proof: Let $X \sim$ Expo($\lambda$). Consider the \textbf{survival function} $P(X\geq s)$.
\begin{align*}
    P(X\geq s) = 1-P(X\leq s) = 1-(1-e^{-\lambda s}) = e^{-\lambda s}
\end{align*}
Then by definition of conditional probability, 
\begin{align*}
    P(X\geq s+t\,|\,X\geq s) = \frac{P(X\geq s+t,\,X\geq s)}{P(X\geq s)}
\end{align*}
But clearly, the term in the denominator is, 
\begin{align*}
    P(X\geq s+t,\,X\geq s) = P(X\geq s+t) = e^{-\lambda(s+t)} = e^{-\lambda s}e^{-\lambda t}
\end{align*}
Substituting this into the previous equation, 
\begin{align*}
    P(X\geq s+t\,|\,X\geq s) = \frac{\cancel{e^{-\lambda s}}e^{-\lambda t}}{\cancel{e^{-\lambda s}}} = e^{-\lambda t} = P(X\geq t)
\end{align*}
From this, we conclude that the probability of surviving $t$ more units of time is independent of how many units of time it has already survived. 

\subsubsection{Gamma}

$X \sim$ Gamma($\alpha, \lambda$). $\alpha$ is called the shape parameter (which varies the shape of the density), and $\lambda$ is called the scale parameter. Can be thought of as the sum of $\alpha$ i.i.d Exp($\lambda$) variables. That is, $X = X_1 + X_2 + \cdots + X_\alpha$, where $X_i\sim$ Exp($\lambda$). Thus, the case of $\alpha=1$ coincides with the exponential density.

Based on the \textbf{gamma function}, which is an extension of the factorial function (to complex numbers). It is defined as follows: 
\begin{align*}
    \Gamma(z) = \int_0^{\infty} x^{z-1}e^{-x}dx,\;\text{where $z>0$}.
\end{align*}
The gamma function has the following identity:
\begin{align*}
    \Gamma(x+1) = x\,\Gamma(x).
\end{align*}
This identity implies that for any positive integer $n$, 
\begin{align*}
    \Gamma(n) = (n-1)!.
\end{align*}

{\huge watch the stat110 video for gamma https://www.youtube.com/watch?v=Qjeswpm0cWY}

\subsubsection{Beta}

{\huge watch the stat110 beta videos https://www.youtube.com/watch?v=UZjlBQbV1KU}

\subsubsection{Standard Normal}

\subsubsection{General Normal}

\subsection{Transformations of Random Variables}

\subsection{Distributions}
pmf/pdf, cdf, ex, var, range of support for k

\begin{landscape}
    \setstretch{2.7}
\begin{tabular}{l l l}
    Bernoulli & Bern($p$) & $p(k) = \begin{cases}
                                    q = 1-p & k = 0 \\
                                    p & k = 1 
                                \end{cases}$ \\

                                Binomial & Bin($n, p$) & $\displaystyle p(k) = {n \choose k}\,p^k\,q^{n-k}$, for $k=1,2,\ldots$ \\
                                Geometric & Geom($p$) & $\displaystyle p(k) = q^{k-1}\,p$, for $k=1,2,\ldots$ \\
                                Negative Binomial & NB($r, p$) & $\displaystyle p(k) = {k-1 \choose r-1}\,p^r\,q^{k-r}$, for $k=1,2,\ldots$; $r\leq k$ \\
                                Hypergeometric & HG($n, r, m$) & $\displaystyle p(k) = \frac{{r \choose k}\,{n-r \choose m-k}}{{n \choose m}}$, for $k\leq m, r \leq n$ \\
                                Poisson & Pois($\lambda$) & $\displaystyle p(k) = \frac{e^{-\lambda}\,\lambda^k}{k!}$, for $k=0,1,\ldots$; $\lambda > 0$ \\[0.2cm]
    \hline
    Uniform & U($a, b$) & $\displaystyle f(x) = \frac{1}{b-a}$, for $a\leq x\leq b$ \\
    Exponential & Exp($\lambda$) & $\displaystyle f(x) = \lambda\,e^{-\lambda\,x}$, for $x\geq 0$ \\
    Gamma & Gamma($\alpha, \lambda$) & $\displaystyle g(t) = \frac{\lambda^\alpha}{\Gamma(\alpha)}\,t^{\alpha - 1}\,e^{-\lambda\,t}$, for $t\geq 0$; $x > 0$a \\
    Beta & Beta($a, b$) & $\displaystyle f(u) = \frac{\Gamma(a+b)}{\Gamma(a)\,\Gamma(b)}\,u^{a-1}\,(1-u)^{b-1}$, for $0\leq u\leq 1$ \\
    Standard Normal & N($0, 1$) & $\displaystyle \varphi(z) = \frac{1}{\sqrt{2\pi}}\,e^{-z^2/2}$ \\
    Normal & N($\mu, \sigma^2$) & $\displaystyle f(x) = \frac{1}{\sigma\sqrt{2\pi}}\,e^{-\left(\frac{x-\mu}{\sigma}\right)^2 / 2} = \frac{1}{\sigma}\varphi\left(\frac{x-\mu}{\sigma}\right)$
\end{tabular}
\setstretch{1}
\end{landscape}
% --------------------------------------------- %
% Expected Values %
% --------------------------------------------- %

\newpage
\section{Expected Values}

\subsection{Mean and Variance}
For two independent random variables $X, Y$, the expectation of the product is the product of the expectation:
\begin{equation*}
    \E(XY) = \E(X)\E(Y)
\end{equation*}

\subsection{Covariance and Correlation}

\subsubsection{Covariance}

Covariance allows us to talk about the variance for sums of random variables. For two jointly distributed random variables $X, Y$, the \textbf{covariance of $X$ and $Y$} is defined as 
\begin{equation*}
    \Cov (X, Y) = \E((X - \E X)(Y - \E Y)).
\end{equation*}

An alternate way to write the covariance is given by 
\begin{align*}
    \Cov (X, Y) &= \E((X - \E X)(Y - \E Y)) \\
                &= \E(XY - Y\E(X) - X\E(Y) + \E(X)\E(Y)) \\
                &= \E(XY) - \E(Y\E(X)) - \E(X \E(Y)) + \E(\E(X) \E(Y)) &\text{by linearity of expectation} \\
                &= \E(XY) - \E(X)\E(Y) - \E(Y)\E(X) + \E(X)\E(Y) &\text{by taking out constants} \\
                &= \E(XY) - \E(X)\E(Y).
\end{align*}

If $X$ and $Y$ are independent, then the covariance is zero. Note that the converse is not true. 

Consider the following cases:
\begin{itemize}
    \item Note that the covariance of some random variable with itself reduces down to its variance:  
        \begin{align*}
            \Cov(X, X) &= \E((X-\E(X))(X-\E(X))) \\
                       &= \E((X-\E(X))^2) \\
                       &= \Var(X).
        \end{align*}

    \item For some constant $c$, the covariance of $X$ and $c$ is zero:
        \begin{align*}
            \Cov(X, c) &= \E((X - \E(X))(c-\E(c))) \\
            &= \E((X-\E(X))(c-c)) \\ 
            &= \E(0) = 0.
        \end{align*}
        
    \item The covariance of a scaled random variable is a scaled covariance:
        \begin{align*}
            \Cov(cX, Y) &= \E((cX - \E(cX))(Y - \E(Y))) \\
            &= \E((cX - c\E(X))(Y - \E(Y))) \\
            &= c\,\E((X - \E(X))(Y - \E(Y))) \\
            &= c\,\Cov(X, y).
        \end{align*}

    \item For random variables $X$, $Y$, and $Z$, the covariance is ``distributive":
        \begin{align*}
            \Cov(X, Y+Z) &= \E((X-\E(X))((Y-\E(Y)) + (Z-\E(Z)))) \\
            &= \E((X-\E(X))(Y-\E(Y)) + (X-\E(X))(Z-\E(Z))) \\
            &= \E((X-\E(X))(Y-\E(Y))) + \E((X-\E(X))(Z-\E(Z))) \\
            &= \Cov(X, Y) + \Cov(X, Z).
        \end{align*}
        More generally, the covariance of sums is
        \begin{align*}
            \Cov\left(\sum_i a_iX_i, \sum_j b_jY_j\right) = \sum_i\sum_ja_ib_j\Cov(X_i, Y_j)
        \end{align*}

\end{itemize}

The covariance can be used to deal with the variance of sums: 
\begin{align*}
    \Var(X+Y) &= \Cov(X+Y, X+Y) \\
    &= \Cov(X, X) + \Cov(X, Y) + \Cov(Y, X) + \Cov(Y, Y) \\
    &= \Var(X) + 2\Cov(X, Y) + \Var(Y).
\end{align*}
Note that if $X$ and $Y$ are independent, then the covariance term becomes zero, and we conclude that the variance of the sum is the sum of the variance. More generally, 
\begin{align*}
    \Var(X_1 + \ldots + X_n) &= \sum_i \Var(X_i) + 2\sum_i \sum_{j \leq i} \Cov(X_i, X_j) \\
    &= \sum_i \Var(X_i) + \sum_{i \neq j} \Cov(X_i, X_j)
\end{align*}

\subsubsection{Correlation}

Given jointly distributed $X$ and $Y$ for which the variances and covariances exist, and the variances are nonzero, the \textbf{correlation of $X$ and $Y$} is defined as 
\begin{align*}
    \rho &= \frac{\Cov(X, Y)}{\SD(X)\SD(Y)} \\
    &= \frac{\Cov(X, Y)}{\sqrt{\Var(X)\Var(Y)}} \\
    &= \Cov\left(\frac{X-\E(X)}{\SD(X)}, \frac{Y-\E(Y)}{\SD(Y)}\right) &\text{(standardize, then take the covariance)}
\end{align*}

The correlation is always between $-1$ and $1$. 

Proof: Assume that $X$ and $Y$ have been standardized. Then $\Var(X) = \Var(Y) = 1$.

\begin{align*}
    0 \leq Var(X+Y) &= \Var(X) + \Var(Y) + 2\Cov(X, Y) = 1 + 1 + 2\rho = 2(1+\rho)\\
    \implies & \rho \geq -1 \\
    0 \leq Var(X-Y) &= \Var(X) + \Var(Y) - 2\Cov(X, Y) = 1 + 1 - 2\rho = 2(1-\rho)\\
    \implies & \rho \leq 1
\end{align*}

\subsubsection{LOTUS}
\subsubsection{Inequalities}

\subsection{Conditional Expectation}

This is the same as expectation, but conditioning on some additional information that we are given. For random variables $X$, $Y$, the \textbf{conditional expectation} of $Y$ given $X$ is, in the discrete case, 
\begin{align*}
    \E(Y|X=x) = \sum_y y\,p_{Y|X}(y|x)
\end{align*}
and in the continuous case,
\begin{align*}
    \E(Y|X=x) = \int_{-\infty}^{\infty} y\,f_{Y|X}(y|x)dy = \int_{-\infty}^{\infty} y\,\frac{f_{X, Y}(x, y)}{f_x(x)}dy
\end{align*}

If the conditional expectation of $Y$ given $X=x$ is defined for all $x$ in the domain of $X$, then the expectation is a well-defined function of $X$. Then we write \textbf($\E(Y|X)$), which is a random variable that is a function of $X$. This computes, for some fixed $X$, the expectation of $Y$. 

\textbf{Law of Total Expectation}, also called \textbf{Law of Iterated Expectation} or \textbf{Adam's Law}: 
\begin{align*}
    \E(Y) = \E(\E(Y|X))
\end{align*}

Proof: Let $g(X) = \E(Y|X)$. Want to show that $\E(Y) = \E(g(X))$. 
\begin{align*}
    \E(g(X)) &= \sum_x g(X)\,P(X=x) &\text{by LOTUS} \\
    &= \sum_x E(Y|X=x)\,P(X=x) &\text{by definition of $g$}\\
    &= \sum_x \left(\sum_y P(Y=y|X=x)\right)\,P(X=x) &\text{by definition of expectation} \\
    &= \sum_y \sum_x y\,P(Y=y, X=x) &\text{by definition of the joint PMF and swapping order of summation} \\
    &= \sum_y y \sum_x P(Y=y, X=x) &\text{since $y$ does not depend on $x$} \\
    &= \sum_y y P(Y=y) &\text{by definition of the marginal PMF} \\
    &= \E(Y) &\text{by definition of expectation}
\end{align*}

Conditional expectations satisfy the following properties:
\begin{itemize}
    \item Taking out what is known: for any function $h$ of $X$, $\E(h(x)\,Y|X) = h(x)\,E(Y|X)$
    \item If $X$, $Y$ are independent, then $\E(Y|X) = \E(Y)$
    \item The \textbf{residual, $Y-\E(Y|X)$}, is uncorrelated with any function of $X$: $\E((Y-\E(Y|X))h(x)) = 0$ 
        
        Proof:
        \begin{align*}
            \E((Y-\E(Y|X))h(x)) &= \E(Yh(x) - \E(Y|X)h(x)) \\
            &= \E(Yh(x)) - \E(\E(Y|X)h(x)) &\text{by linearity} \\
            &= \E(Yh(x)) - \E(\E(Yh(x)|X)) &\text{by the reverse of the first property} \\
            &= \E(Yh(x)) - \E(Yh(x)) &\text{by Iterated Expectation} \\
            &= 0
        \end{align*}
\end{itemize}

The \textbf{conditional variance} of $Y$ given $X$ is the same as the normal variance but conditioned on $X$ wherever appropriate:
\begin{align*}
    \Var(Y|X) = \Var(Y^2|X) - (\Var(Y|X))^2 = \E((Y-\E(Y|X))^2|X)
\end{align*}

\textbf{Law of Total Variance}, also called \textbf{Law of Iterated Variance} or \textbf{Eve's Law}:
\begin{align*}
    \Var(Y) = \Var(\E(Y|X)) + \E(\Var(Y|X))
\end{align*}

Proof: Just as we defined $\E(Y|X)$ before to be a random variable as a function of $X$, we can do the same with $\Var(Y|X)$ if $\Var(Y|X=x) = E(Y^2|X=x) - (\E(Y)|X=x)^2$ is defined for all $x$ in the domain of $X$. Then 
\begin{align*}
    &\Var(Y|X) = \E(Y^2|X) - (\E(Y|X))^2 &\text{by definition of variance} \\
    \implies &\E(\Var(Y|X)) = \E(\E(Y^2|X)) - \E(\E(Y|X))^2) &\text{by linearity of expectation}
\end{align*}
and 
\begin{align*}
    \Var(\E(Y|X)) = \E((\E(Y|X))^2) - (\E(\E(Y|X)))^2 &\qquad\text{by definition of variance}
\end{align*}
and 
\begin{align*}
    \Var(Y) &= \E(Y^2) - (\E(Y))^2 &\text{by definition of variance} \\ 
    &= \E(\E(Y^2|X)) - (\E(\E(Y|X)))^2 &\text{by iterated expectation}
\end{align*}
Then 
\begin{align*}
    \Var(Y) &= \E(\E(Y^2|X)) - (\E(\E(Y|X)))^2
\end{align*}
If we add and subtract $\E((\E(Y|X))^2)$, this expression remains the same:
\begin{align*}
    \Var(Y) &= \E(\E(Y^2|X)) - \E((\E(Y|X))^2) + \E((\E(Y|X))^2) - (\E(\E(Y|X)))^2
\end{align*}
Then by grouping the first two and the last two terms together:
\begin{align*}
    \Var(Y) &= \E(\Var(Y|X)) + \Var(\E(Y|X))
\end{align*}



\subsection{Moment Generating Functions}

A \textbf{moment generating function (MGF)} is an alternate way of specifying the distribution of a function (apart from the pdf/cdf). For some random variable $X$, the MGF is defined as 
\begin{align*}
    M_X(t) := E(e^{tX}).
\end{align*}
wherever the expectation exists. The MFG \textit{uniquely determines} the probability distribution if it exists in some open interval around zero. 

The \textbf{n-th moment} is defined as $E(X^n)$, and the \textbf{n-th central moment} is defined as $E\left(\left(X-E(X)\right)^n\right)$.

By these definitions, it is clear where the "generating" part of the name comes from; the Maclaurin expansion of $e^{tX}$ is 
\begin{equation*}
    e^{tX} = 1 + tX + \frac{t^2X^2}{2!} + \frac{t^3X^3}{3!} + \ldots,
\end{equation*}
and thus, by linearity of expectation, 
\begin{align*}
    E(e^{tX}) &= 1 + tE(X) + \frac{t^2E(X^2)}{2!} + \frac{t^2E(X^3)}{3!} + \ldots \\
              &= 1 + tm_1 + \frac{t^2}{2!}m_2 + \frac{t^3}{3!}m_3 + \ldots 
\end{align*}
where $m_i$ is the $i$th moment. 

Thus, the $i$th moment is the coefficient of $\frac{t^i}{i!}$, and it can be found by taking the $i$th derivative of the series expansion (lower powered terms will disappear, and higher powered terms will equal zero when evaluated at zero).  

For random variables $X$ with MGF $M_X$ and $Y$ with MGF $M_Y$, if $X$ and $Y$ are independent, then the MGF of $X+Y$ is 
\begin{align*}
    M_{X+Y}(t) &= E(E^{t(X+Y)}) \\
               &= E(e^{tX+tY}) \\
               &= E(e^{tX}e^{tY}) \\
               &= E(e^{tX})E(e^{tY}) &\text{by independence} \\
               &= M_X(t)M_Y(t).
\end{align*}

For random variable $X$ with MGF $M_X$ and random variable $Y=a+bX$, $Y$ has MGF 
\begin{align*}
    M_Y(t) &= E(e^{tY}) \\
           &= E(e^{t(a+bX)}) \\
           &= E(e^{at + btX}) \\
           &= E(e^{at}e^{btX}) \\
           &= e^{at}E(btX) \\
           &= e^{at}M_X(bt).
\end{align*}

\subsection{Equations}
means, variances, and mgfs for each distribution. 

% --------------------------------------------- %
% Joint Distributions %
% --------------------------------------------- %

\newpage
\section{Joint Distributions}

\subsection{Joint and Marginal Distributions}

Joint distributions deal with probabilities of multiple random variables defined on the same sample space.

For random variables $X, Y$, the \textbf{joint CDF} of $X$ and $Y$ is \begin{equation*}
    F(x, y) = P(X=x,\,Y=y).
\end{equation*}

The probability that $(X, Y)$ is in some rectangle $[x_1, x_2] \times [y_1, y_2]$ is given by 
\begin{equation*}
    P(x_1\leq X\leq x_2,\,y_1\leq Y\leq y_2) = F(x_2, y_2) - F(x_1, y_2) - F(x_2, y_1) + F(x_1, y_1).
\end{equation*}

\begin{figure}[!ht]
    \centering
    \fig{joint}
    \caption{The probability of the shaded rectangle can be found by taking the probability of the $(x_2, y_2)$ rectangle, subtracting the probabilities of the $(x_1, y_2)$ and $(x_2, y_1)$ rectangles, and then adding back in the probability of the $(x_1, y_1)$ rectangle to account for the double subtraction.}
\end{figure}

\subsubsection{Discrete}

Suppose $X$ and $Y$ are discrete random variables defined on the same sample space, with $X$ taking on values $x_1, x_2, \ldots$, and Y taking on values $y_1, y_2, \ldots$. Then the \textbf{joint pmf $p(x, y)$} is 
\begin{equation*}
    p(x_i, y_i) = P(X=x_i,\,Y=y_i).
\end{equation*}

The pmf of one random variable is called its \textbf{marginal probability function}. For $X$ and $Y$ jointly distributed, the marginal pmf of $X$ is given by 
\begin{equation*}
    p_X(x) = \sum_i p(x, y_i).
\end{equation*}

In general, to get a marginal pmf, fix a value for the random variable whose marginal pmf we want, and then sum the joint pmf over all possible values for all other (jointly distributed) random variables. That is, for $X_1, \ldots, X_n$ jointly distributed, 
\begin{equation*}
    p_{X_i}(x_i) = \sum_{j:x_j\neq x_i} p(x_1, \ldots, x_n).
\end{equation*}

For example, the two-dimensional marginal pmf for $X_1$ and $X_2$ would be given by 
\begin{equation*}
    p_{X_1, X_2}(x_1, x_2) = \sum_{x_3\ldots x_n} p(x_1, \ldots, x_n).
\end{equation*}

\subsubsection{Continuous}

Suppose $X$ and $Y$ are continuous random variables. For a set $A$ defined as $A={(X, Y)\,|\,X\leq x,\,Y\leq y}$, the \textbf{joint cdf} can be expressed as
\begin{equation*}
    F(x, y) = \int_{-\infty}^x \int_{-\infty}^y f(u, v)\,dv\,du.
\end{equation*}

Then by the Fundamental Theorem of Calculus, the \textbf{joint density} is given by 
\begin{equation*}
    f(x, y) = \partial_x\partial_y F(x, y)
\end{equation*}
wherever the derivative is well-defined. The \textbf{joint density} $f(x, y)$ satisfies the following properties:
\begin{itemize}
    \item $f(x, y) \geq 0$ (non-negative)
    \item $\displaystyle \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x, y) \,dx\,dy = 1$.
\end{itemize}

For some two-dimensional set $A$, 
\begin{equation*}
    P((X, Y) \in A) = \iint \limits_A f(x, y) \,dx\,dy.
\end{equation*}

The \textbf{marginal cdf} of $X$, denoted $F_X$, is 
\begin{equation*}
    F_X(x) = P(X\leq x) = \lim_{y\to\infty}F(x, y) = \int_{-\infty}^x \int_{-\infty}^{\infty}f(u, y)\,dy\,du,
\end{equation*}
and the \textbf{marginal density} of $X$, denoted $f_x$, is 
\begin{equation*}
    f_X(x) = F'_X(x) = \int_{-\infty}^{\infty}f(x, y)\,dy 
\end{equation*}


\subsection{Independence in Joint Distributions}

\subsection{Conditional Distributions}
\subsubsection{Discrete}
\subsubsection{Continuous}

\subsection{Functions of Joint Distributions}

\subsection{Order Statistics}

\subsection{Equations}

\section{Limit Theorems}

\subsection{Law of Large Numbers}

Given i.i.d.\ random variables $X_1, X_2, \ldots$, each with mean $\mu$ and variance $\sigma^2$, let the sample mean be defined as $\overline{X_n} = \frac{1}{n}\sum_{i=1}^{n}X_i$. Then for $\epsilon > 0$ the \textbf{weak law of large numbers} says that , 
\begin{align*}
    P(|\overline{X_n} - \mu| > \epsilon) \to 0~\text{as $n \to \infty$}
\end{align*}
In other words, as the sample size increases, it is increasingly likely that the sample mean is close to the true mean. 

Proof: Note that
\begin{align*}
    \Var(\overline{X_n}) &= \Var(\frac{1}{n}\sum_{i=1}^{n}X_i) &\text{by definition of $\overline{X_n}$} \\
    &= \frac{1}{n^2}\Var(\sum_{i=1}^{n}X_i) &\text{by pulling out the $\frac{1}{n}$} \\
    &= \frac{1}{n^2}n\sigma^2 &\text{by independence}
    &= \frac{\sigma^2}{n}
\end{align*}

Then 
\begin{align*}
    P(|\overline{X_n} - \mu| > \epsilon) &\leq \frac{\Var(\overline{X_n})}{\epsilon^2} &\text{by Chebyshev's inequality} \\
    &= \frac{\sigma^2}{n\epsilon^2} \to 0 &\text{as $n \to \infty$}
\end{align*}
\end{document}
